{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to download the dataset from kaggle: [Dataset](https://www.kaggle.com/datasets/mirzahasnine/loan-data-set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gYFkj8lmILY"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle pyswarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iYTyw4vmRI8",
        "outputId": "788d6fb6-bb49-41b0-9ce0-00b8605889b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading loan-data-set.zip to /content\n",
            "  0% 0.00/10.9k [00:00<?, ?B/s]\n",
            "100% 10.9k/10.9k [00:00<00:00, 3.98MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ! mkdir ~/.kaggle\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets download mirzahasnine/loan-data-set --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSx22oZ5nOeJ",
        "outputId": "06f7a8a8-c6ad-4831-a292-2fa9ee7f065d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "2023-04-19 12:29:02,551 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
            "pyswarms.single.global_best: 100%|██████████|50/50, best_cost=-.691\n",
            "2023-04-19 12:29:29,559 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -0.6909090909090909, best pos: [1.35791274 1.60296745 0.31250161]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized hyperparameters: n_neighbors=14, weights='distance', p=4.125016082400592\n",
            "Test accuracy: 69.09%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pyswarms.single import GlobalBestPSO\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load your dataframe\n",
        "df = pd.read_csv(\"loan_train.csv\")\n",
        "\n",
        "# Get a list of all string columns\n",
        "str_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create an instance of the OneHotEncoder class\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the string columns in the dataframe\n",
        "encoded_cols = encoder.fit_transform(df[str_cols])\n",
        "\n",
        "# Get the feature names of the encoded columns\n",
        "feature_names = encoder.get_feature_names_out(str_cols)\n",
        "\n",
        "# Convert the encoded columns back to a dataframe and merge it with the original dataframe\n",
        "encoded_df = pd.DataFrame(encoded_cols, columns=feature_names)\n",
        "loan_df = pd.concat([df.drop(str_cols, axis=1), encoded_df], axis=1)\n",
        "\n",
        "loan_df = loan_df.dropna()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(loan_df.iloc[:, :-1], loan_df.iloc[:, -1], test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Define the function to optimize\n",
        "def knn_cv_acc(hyperparams):\n",
        "    if hyperparams.ndim == 1:\n",
        "        # Convert hyperparameters to integers\n",
        "        n_neighbors = abs(int(hyperparams[0] * 10)) + 1\n",
        "        weights = 'uniform' if int(hyperparams[1]) == 0 else 'distance'\n",
        "        p = abs(int(hyperparams[2] * 10)) + 1\n",
        "    else:\n",
        "        # Convert hyperparameters to integers for all particles\n",
        "        n_neighbors = [abs(int(h[0] * 10)) + 1 for h in hyperparams]\n",
        "        weights = ['uniform' if int(h[1]) == 0 else 'distance' for h in hyperparams]\n",
        "        p = [abs(float(h[2] * 10)) + 1 for h in hyperparams]\n",
        "\n",
        "    # Create KNN classifiers with the given hyperparameters\n",
        "    knns = [KNeighborsClassifier(n_neighbors=n, weights=w, p=pi) for n, w, pi in zip(n_neighbors, weights, p)]\n",
        "\n",
        "    # Train the classifiers and get the accuracy on the validation set\n",
        "    accs = []\n",
        "    for knn in knns:\n",
        "        knn.fit(X_train, y_train)\n",
        "        acc = knn.score(X_test, y_test)\n",
        "        accs.append(acc)\n",
        "\n",
        "    # Return the negative accuracies (to be minimized by PSO)\n",
        "    return [-acc for acc in accs]\n",
        "\n",
        "\n",
        "# Define the upper and lower bounds for the hyperparameters\n",
        "lb = [1, 0, 1]  # lower bounds for n_neighbors, weights, and p\n",
        "ub = [10, 1, 3]  # upper bounds for n_neighbors, weights, and p\n",
        "\n",
        "# Call the PSO optimizer\n",
        "optimizer = GlobalBestPSO(n_particles=10, dimensions=3, options={\"c1\": 0.5, \"c2\": 0.3, \"w\": 0.9})\n",
        "xopt, fopt = optimizer.optimize(knn_cv_acc, iters=50)\n",
        "# Print the optimized hyperparameters and the corresponding test accuracy\n",
        "n_neighbors = abs(int(fopt[0] * 10)) + 1\n",
        "weights = 'uniform' if int(fopt[1]) == 0 else 'distance'\n",
        "p = abs(float(fopt[2] * 10)) + 1\n",
        "print(\"Optimized hyperparameters: n_neighbors={}, weights='{}', p={}\".format(n_neighbors, weights, p))\n",
        "knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p)\n",
        "knn.fit(X_train, y_train)\n",
        "acc = knn.score(X_test, y_test)\n",
        "print(\"Test accuracy: {:.2f}%\".format(acc * 100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
